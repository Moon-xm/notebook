{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perplexity是用于评测Language Model性能的一个指标。表现为一个数值，通常数值越小，说明LM的性能越好。即模型预测分布越接近自然语言的真实分布。\n",
    "\n",
    "要理解perplexity需要引入一些香农的香农信息论中的概念。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "\n",
    "1948年香农提出的信息熵解决了信息的量化度量的问题，信息熵用于描述信息的不确定度。假设$X$为随机变量，$P(X)$为该变量取某一个值得概率，那么\n",
    "$$\n",
    "H(X) = E[I(x_i)] = -\\displaystyle\\sum P(x_i)log_2(P(x_i))\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy\n",
    "\n",
    "在信息论中，交叉熵表示两个概率分布P、Q(P表示真实分布，Q表示非真实分布)在相同一组事件中使用Q分布来表示P分布所需要的平均编码长度.\n",
    "\n",
    "对于离散型分布\n",
    "$$\n",
    "H(P, Q) = \\displaystyle\\sum P(i)log_2(\\frac{1}{Q(i)})\n",
    "$$\n",
    "对于连续型分布\n",
    "$$\n",
    "H(P, Q) = -\\int_XP(x)log_2(Q(x))dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy rate\n",
    "\n",
    "对于索引可数的随机过程 $\\chi = X_1,X_2,\\ldots,X_n$ ,其熵率定义为:\n",
    "$$\n",
    "H(\\chi) = \\lim_{n\\to\\infty}\\frac{1}{n}H(X_1, X_2, \\ldots,X_n)\\tag{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于独立同分布的序列，各随机变量的熵相等，则显然有：\n",
    "$$\n",
    "H(\\chi) = \\lim_{n\\to\\infty}\\frac{1}{n}\\displaystyle\\sum_{i = 1}^{n}H(X_i) = H(X_i)\\tag{2}\n",
    "$$\n",
    "所以熵率依其定义可粗略地类比于随机过程中 *per-random_variable* *entropy*。比如，考虑这样一台打字机，假设可输出$m$个等可能的符号。因此打字机可产生长度为$n$的共$m^n$个序列，并且都等可能出现。那么对于该打印机\n",
    "$$\n",
    "H(\\chi) = log_2 m\\tag{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy of Language and Cross Entropy of Language\n",
    "\n",
    "在语言模型中，对于语言$L$中的长度为的$n$的单词序列单词$S = {w_1, w_2, \\ldots,w_n}$序列的熵的计算公式为 \n",
    "$$\n",
    "H(w_1, \\ldots, w_n) = -\\sum P(S)log_2P(S)\\tag{4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么在单词序列熵的基础上，根据熵率的定义，可粗略的定义 *per-word entropy*：\n",
    "$$\n",
    "\\frac{1}{n}H(w_1, \\ldots, w_n) = -\\frac{1}{n}\\sum P(S)log_2P(S)\\tag{5}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而若将语言 ![[公式]](https://www.zhihu.com/equation?tex=L) 视作随机过程，则有语言$L$的熵率：\n",
    "$$\n",
    "H(L) = -\\lim_{n\\to\\infty}\\frac{1}{n}\\displaystyle\\sum_{w_1,\\ldots,w_n\\in L}P(w_1,\\ldots,w_n)log_2P(w_1,\\ldots,w_n)\\tag{6}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于上式，根据Shannon-McMillan-Breiman theorem又可以做如下近似：\n",
    "$$\n",
    "H(L) = -\\lim_{n\\to\\infty}\\frac{1}{n}log_2P(w_1,\\ldots,w_n)\\tag{7}\n",
    "$$\n",
    "那么，计算在语言$L$熵的交叉熵率为：\n",
    "$$\n",
    "H(P, Q) = -\\lim_{n\\to\\infty}\\frac{1}{n}\n",
    "\\displaystyle\\sum_{L}P(w_1, \\ldots,w_n)log_2Q(w_1, \\ldots, w_n)\\tag{8}\n",
    "$$\n",
    "上式可以近似为\n",
    "$$\n",
    "H(P, Q) = -\\lim_{n\\to\\infty}\\frac{1}{n}\n",
    "\\displaystyle\\sum_{L}log_2Q(w_1, \\ldots, w_n)\\tag{9}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity\n",
    "\n",
    "通过上面的内容，足以引入perplexity了。假设$P(S)$为语言模型计算出S的概率，那么对于整个语言L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Log-likehood\n",
    "  $$\n",
    "  LL(L) = \\displaystyle\\sum_{S\\in L}log_2P(S)\\tag{10}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Per-word log-likehood\n",
    "  $$\n",
    "  WLL(L) = \\frac{1}{\\sum_{S\\in L}|S|}\\displaystyle\\sum_{S\\in L}log_2P(S)\\tag{11}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Per-word cross likehood\n",
    "  $$\n",
    "  H(L) = - \\frac{1}{\\sum_{S\\in L}|S|}\\displaystyle\\sum_{S\\in L}log_2P(S)\\tag{12}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perplexity \n",
    "  $$\n",
    "  PPL(L) = 2^{H(L)} = 2^{-WLL(L)}\\tag{13}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shannon-McMillan-Breiman theorem\n",
    "\n",
    "//todo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
