{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 认识Pytorch中的Tensor\n",
    "## 什么是Tensor?\n",
    "Tensor在程序中，类似于Numpy中的ndarrays,并且Tensor可以使用GPU进行计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.3870e-33, 6.8608e+22, 1.8497e+31],\n",
      "        [6.1101e-04, 4.8419e+30, 1.7256e+25],\n",
      "        [1.2118e+25, 1.2039e+30, 2.9777e+35],\n",
      "        [8.9036e-15, 1.9431e-19, 1.8987e+28],\n",
      "        [7.1061e+31, 4.2964e+24, 3.0607e+32]])\n"
     ]
    }
   ],
   "source": [
    "# 构建一个5 * 3的矩阵，不初始化\n",
    "# 通过观察加过可以发现，每一次的初始值是随机的\n",
    "# 实际上，在pytorch1.4版本中，个人认为torch.empty()方法和torch.rand()作用一样\n",
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7639, 0.3414, 0.6534],\n",
      "        [0.7058, 0.1155, 0.3478],\n",
      "        [0.2522, 0.2480, 0.3909],\n",
      "        [0.4598, 0.5333, 0.9211],\n",
      "        [0.4702, 0.2319, 0.0703]])\n"
     ]
    }
   ],
   "source": [
    "# 但是与empty()方法不同的是，rand()生成的数值在0~1\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# 构造一个全为0的矩阵，且数据类型为long\n",
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "# 当然也可以直降将Python list对象转化成一个Tensor,这种操作与numpy.array()一样\n",
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[5.5243e-01, 9.8879e-01, 9.9122e-01],\n",
      "        [6.1828e-01, 7.7484e-01, 5.6105e-01],\n",
      "        [2.9816e-03, 5.3450e-01, 8.1157e-01],\n",
      "        [2.7998e-01, 2.5154e-01, 9.2710e-01],\n",
      "        [2.0629e-01, 9.3400e-05, 7.2138e-01]])\n"
     ]
    }
   ],
   "source": [
    "# 可以基于已有的Tensor构建一个新的Tensor\n",
    "x = x.new_ones(5, 3, dtype=torch.double)\n",
    "print(x)\n",
    "x = torch.rand_like(x, dtype=torch.float)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "<class 'torch.Size'>\n"
     ]
    }
   ],
   "source": [
    "# 通过Tensor对象的size()方法可以获取其形状\n",
    "print(x.size())\n",
    "print(type(x.size()))\n",
    "# 值得注意的是，torch.Size是一个tuple,支持所有的tuple操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多个Tensor的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9659, 1.5883, 1.5235],\n",
      "        [1.1187, 0.9180, 1.5058],\n",
      "        [0.8229, 0.5997, 1.0227],\n",
      "        [1.1775, 0.5293, 1.1344],\n",
      "        [0.7374, 0.3774, 0.9909]])\n",
      "tensor([[0.9659, 1.5883, 1.5235],\n",
      "        [1.1187, 0.9180, 1.5058],\n",
      "        [0.8229, 0.5997, 1.0227],\n",
      "        [1.1775, 0.5293, 1.1344],\n",
      "        [0.7374, 0.3774, 0.9909]])\n",
      "tensor([[0.9659, 1.5883, 1.5235],\n",
      "        [1.1187, 0.9180, 1.5058],\n",
      "        [0.8229, 0.5997, 1.0227],\n",
      "        [1.1775, 0.5293, 1.1344],\n",
      "        [0.7374, 0.3774, 0.9909]])\n",
      "tensor([[0.9659, 1.5883, 1.5235],\n",
      "        [1.1187, 0.9180, 1.5058],\n",
      "        [0.8229, 0.5997, 1.0227],\n",
      "        [1.1775, 0.5293, 1.1344],\n",
      "        [0.7374, 0.3774, 0.9909]])\n"
     ]
    }
   ],
   "source": [
    "# 加法 method 1\n",
    "y = torch.rand(5, 3) # 令x与y的形状相同\n",
    "print(x + y)\n",
    "\n",
    "# 加法 method 2\n",
    "print(torch.add(x, y))\n",
    "\n",
    "# 加法 method 3,提供一个输出Tensor作为参数，即指定操作结果的存放变量\n",
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)\n",
    "\n",
    "# 当然也可以实现将Tensor x的值加到Tensor的所有对应位置上\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.8879e-01, 7.7484e-01, 5.3450e-01, 2.5154e-01, 9.3400e-05])\n",
      "torch.Size([5])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# 我们也可以使用类似Numpy的索引操作\n",
    "print(x[:, 1])\n",
    "print(x[:, 1].size())\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "# 如果想改变一个Tensor的形状，可以使用torch.view()方法\n",
    "x = torch.rand(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8) # 这个地方的-1，代表占位，它具体的值可以由其他维度的值推断出来\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6676, 0.3595, 0.7837, 0.1519],\n",
      "        [0.9069, 0.0993, 0.0733, 0.6308],\n",
      "        [0.7212, 0.0625, 0.7546, 0.0824],\n",
      "        [0.5844, 0.2093, 0.0984, 0.2156]])\n"
     ]
    }
   ],
   "source": [
    "# 必须警惕的一点是，view中的参数相乘，等于原来Tensor的元素个数才行\n",
    "print(x.view(4, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1163])\n",
      "0.11626172065734863\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# 如果有一个只有一个元素的Tensor(这种Tensor被称为标量),那么可以使用item()方法获取其数值\n",
    "x = torch.rand(1)\n",
    "print(x)\n",
    "print(x.item())\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n"
     ]
    }
   ],
   "source": [
    "# 默认情况下，torch.cat操作沿着最高维连接\n",
    "x_1 = torch.randn(3, 5)\n",
    "x_2 = torch.randn(2, 5)\n",
    "y = torch.cat([x_1, x_2])\n",
    "print(y.size())\n",
    "# torch.Size([5, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8])\n"
     ]
    }
   ],
   "source": [
    "# 当然我们也可以指定沿着哪个维度，例如我们使用convolution处理文本数据的时候，需要多个filter并行\n",
    "# 最后的feature map需要cat操作\n",
    "x_1 = torch.randn(3, 5)\n",
    "x_2 = torch.randn(3, 3)\n",
    "y = torch.cat([x_1, x_2], 1)\n",
    "# 注意torch.cat 第二个参数，它代表要扩展维度的下标\n",
    "print(y.size())\n",
    "# torch.Size([3, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 仿射变换\n",
    "深度学习最核心的内容之一就是仿射变换，它其实是线性变化，在数学上的表达式为\n",
    "$$f(x) = A\\vec{x} + b$$\n",
    "但是在深度学习上，每一个样本以行的形式存在的，所以实际上的操作是\n",
    "$$f(x) = \\vec{x}\\cdot A + b$$\n",
    "值得注意的是多个仿射变换仍然是仿射变换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2304,  0.9203, -0.3519],\n",
      "        [-1.2158,  0.5064, -0.5361],\n",
      "        [ 0.7637, -0.1092,  0.5906]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 定义一个从五维到三维的仿射\n",
    "line = torch.nn.Linear(5, 3)\n",
    "data = torch.randn(3, 5)\n",
    "print(line(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 非线性变换\n",
    "由上面的多个仿射组合仍然是仿射可知，在深度学习中，如果仅仅是靠线性层的堆叠，实际上和一层线性变化没有区别，那么就需要引入非线性变换。非线性函数就只指在不同的点上微分结果不同\n",
    "<br>\n",
    "常见的非线性函数非常多，在深度学习上主要有sigmoid、Tanh、ReLU等等，选择这几个函数主要是这几个函数偏导数容易求,以sigmoid为例\n",
    "$$\\frac{\\partial \\sigma}{\\partial x} = \\sigma(x)(1 - \\sigma(x))$$\n",
    "在实际的深度学习模型中，应该尽量减少sigmoid的时候，这是因为$\\sigma(x)$取值逼近0或者1的时候，求得梯度非常小，那么模型难以收敛到最优状态。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor自动求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch中的autograd包可以为Tensor对象上的所有操作提供自动微分。\n",
    "<br>\n",
    "torch.Tensor包是Pytorch的核心，一切网络都围绕它展开。如何设置Tensor对象的required_grad属性为True,那么Pytorch将会追踪在该对象上的所有操作。完成计算后，调用Tensor类的backward()方法来自动计算所有的梯度。该Tensor对象的梯度被累计到其grad属性中。\n",
    "<br>\n",
    "<br>\n",
    "如何要停止tensor历史记录的跟踪，可以调用Tensor类的detach()方法，它将该Tensor从计算图中剥离，但变量仍指向原始内存地址，它的具体操作是设置grad_fn属性为None，requires_grad属性设置为False\n",
    "<br>\n",
    "还有一种方法是使用$with\\ torch.no\\_grad()$把代码块包装起来，这个方法在评估模型的时候用得比较多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在使用backward()自动求梯度的时候，假设操作是`x.backward(args)`，如果x是一个标量，那么不需要指定任何参数，如果不是，则需要一个gradient参数来指定张量的形状。(这个参数的作用会在后面解释)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 创建一个张量，设置requires_grad = True来跟踪与它相关的计算\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "# y作为操作的结果创建，所以它有grad_fn属性。\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backward参数的解释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [2., 3.]])\n",
      "tensor([5., 8.], grad_fn=<SqueezeBackward3>)\n",
      "tensor([5., 8.])\n"
     ]
    }
   ],
   "source": [
    "# 先给出程序示例\n",
    "x = torch.tensor([1., 2.], requires_grad=True)\n",
    "y = torch.tensor([[1., 2.], [2., 3.]])\n",
    "print(y)\n",
    "z = x@y\n",
    "print(z)\n",
    "gradient_vector = torch.tensor([1., 2.], dtype=torch.float)\n",
    "z.backward(gradient_vector)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述代码断中$\\vec{a} = (1, 2)$, 变量y为一个矩阵$\\begin{bmatrix}\n",
    "   1 & 2 \\\\\n",
    "   2 & 3\n",
    "\\end{bmatrix}$,z为$[1, 2] \\cdot\\begin{bmatrix}\n",
    "   1 & 2 \\\\\n",
    "   2 & 3\n",
    "\\end{bmatrix} = [5, 8]$，\n",
    "那么$\\frac{\\partial\\vec{z}}{\\partial\\vec{a}} = \\begin{bmatrix}\n",
    "   \\frac{\\partial z_1}{\\partial a_1} & \\frac{\\partial z_2}{\\partial a_2} \\\\\n",
    "   \\frac{\\partial z_1}{\\partial a_1} & \\frac{\\partial z_2}{\\partial a_2}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "   1 & 2 \\\\\n",
    "   2 & 3\n",
    "\\end{bmatrix}$,而gradient参数为$[1, 2]$，实际输出的结果为$\\vec{gradient}\\cdot \\frac{\\partial\\vec{z}}{\\partial\\vec{a}}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chenguang",
   "language": "python",
   "name": "chenguang"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
