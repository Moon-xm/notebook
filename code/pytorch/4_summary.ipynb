{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch的核心是两个特征\n",
    "1. 一个N维张量，类似于numpy，但是可以在GPU上运算\n",
    "2. 搭建和训练神经网络是的自动微分、求导机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 35382166.98289068\n",
      "1 28152083.021180052\n",
      "2 24975552.184205044\n",
      "3 21719252.14714821\n",
      "4 17214528.916116476\n"
     ]
    }
   ],
   "source": [
    "# 在正式使用pytorch之前，回顾一下numpy的一些用法。\n",
    "import numpy as np\n",
    "\n",
    "batch_size, input_size, hidden_size, output_size = 64, 1000, 100, 10\n",
    "\n",
    "# 创建随机的输入以及输出\n",
    "x = np.random.randn(batch_size, input_size)\n",
    "y = np.random.randn(batch_size, output_size)\n",
    "\n",
    "# 随机初始化权重\n",
    "w1 = np.random.randn(input_size, hidden_size)\n",
    "w2 = np.random.randn(hidden_size, output_size)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(5):\n",
    "    # 前向传播\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "    # 反向传播\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 27933208.0\n",
      "1 24945064.0\n",
      "2 24965536.0\n",
      "3 24551664.0\n",
      "4 21835740.0\n",
      "5 16774729.0\n",
      "6 11234487.0\n",
      "7 6828489.0\n",
      "8 4039985.0\n",
      "9 2469799.5\n"
     ]
    }
   ],
   "source": [
    "# Numpy是一个优秀的矩阵运算工具，但是不能利用numpy来加速其数值运算\n",
    "# 本例使用tensor将在随机数据上训练一个两层的网络，与前面的示例类似，仅仅使用tensor\n",
    "import torch\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"CPU\")\n",
    "\n",
    "batch_size, input_size, hidden_size, output_size = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(batch_size, input_size, device=device, dtype=dtype)\n",
    "y = torch.randn(batch_size, output_size, device=device, dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(input_size, hidden_size, device=device, dtype=dtype)\n",
    "w2 = torch.randn(hidden_size, output_size, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(10):\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    print(t, loss)\n",
    "    grad_y_pred = 2.0 * (y_pred -y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 24254692.0\n",
      "1 17557514.0\n",
      "2 14198531.0\n",
      "3 12105104.0\n",
      "4 10384138.0\n",
      "5 8770156.0\n",
      "6 7159100.0\n",
      "7 5669248.0\n",
      "8 4352951.0\n",
      "9 3288865.0\n"
     ]
    }
   ],
   "source": [
    "# 上述示例使用手动对每一层的参数进行更新，实际使用pytorch的模型的时候，肯定不能这样做，\n",
    "# pytorch提供了自动求导工具\n",
    "\n",
    "x = torch.randn(batch_size, input_size, device=device, dtype=dtype)\n",
    "y = torch.randn(batch_size, output_size, device=device, dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(input_size, hidden_size, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(hidden_size, output_size, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(10):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "    loss.backward()\n",
    "    # 防止梯度更新的时候更新pytorch计算图\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们也可以定义自己的自动求导函数\n",
    "class MyRule(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return x.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, = ctx.saved_tensors\n",
    "        grad_x = grad_output.clone()\n",
    "        grad_x[x < 0] = 0\n",
    "        return grad_x\n",
    "    \n",
    "# 我们使用MyRule.apply(argu_tensor)就可以了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.1464388370513916\n",
      "1 1.1463005542755127\n",
      "2 1.1461621522903442\n",
      "3 1.1460238695144653\n",
      "4 1.1458854675292969\n",
      "5 1.145747423171997\n",
      "6 1.1456091403961182\n",
      "7 1.1454709768295288\n",
      "8 1.14533269405365\n",
      "9 1.14519464969635\n"
     ]
    }
   ],
   "source": [
    "# 使用大规模的网络中，使用autograd包太过于底层，也会极大地增加代码难度。\n",
    "# torch.nn包为我们提供了高级API，方便我们如何使用Keras一样快速地构架模型\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(input_size, hidden_size),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hidden_size, output_size)\n",
    ")\n",
    "model.to(device)\n",
    "loss_fn = torch.nn.MSELoss(reduce=\"mean\")\n",
    "learning_rate = 1e-4\n",
    "for t in range(10):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.1450566053390503\n",
      "1 1.1172531843185425\n",
      "2 1.0901139974594116\n",
      "3 1.0637024641036987\n",
      "4 1.0379905700683594\n",
      "5 1.0131222009658813\n",
      "6 0.9890559315681458\n",
      "7 0.9658142328262329\n",
      "8 0.9431743621826172\n",
      "9 0.9212177395820618\n"
     ]
    }
   ],
   "source": [
    "# 实际上自己一层一层参数进行梯度更新也是很麻烦的一件事，pytorch为我们提供了更好的选择\n",
    "# 仍然使用上一个cell定义的模型\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(10):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred,  y)\n",
    "    print(t, loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 控制流和权重共享\n",
    "作为动态图和权重共享的一个列子，下面将实现一个非常奇怪的模型：一个全连接的ReLU网络，在每一次前向传播时，它的隐藏层的数目为1到4的随机数，这样可以多次重用相同的权重连接来计算."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.079433560371399\n",
      "1 1.138946771621704\n",
      "2 1.0769429206848145\n",
      "3 1.138685941696167\n",
      "4 1.0794172286987305\n",
      "5 1.0873134136199951\n",
      "6 1.0794012546539307\n",
      "7 1.0872423648834229\n",
      "8 1.0793811082839966\n",
      "9 1.137485384941101\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_layer = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.hidden_layer = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.output_layer = torch.nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_relu = self.input_layer(x).clamp(min=0)\n",
    "        for _ in range(random.randint(0, 3)):\n",
    "            h_relu = self.hidden_layer(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_layer(h_relu)\n",
    "        return y_pred\n",
    "    \n",
    "batch_size, input_size, hidden_size, output_size = 64, 1000, 100, 10\n",
    "x = torch.randn(batch_size, input_size)\n",
    "y = torch.randn(batch_size, output_size)\n",
    "\n",
    "model = DynamicNet(input_size, hidden_size, output_size)\n",
    "criterion = torch.nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "\n",
    "for t in range(10):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型加载与保存\n",
    "在保存和加载模型的时候，一定要熟悉pytorch的三个核心\n",
    "- torch.save将序列化对象保存到磁盘，此函数使用的Python内置的pickle模块进行序列化，可以用来保存模型，Tensor，字典等等。\n",
    "- torch.load使用pickle中的unpickling功能，将pickle对象文件反序列化到内存中。还可以用于设备加载数据。\n",
    "- torch.nn.Module.load_state_dict()使用反序列化函数反序列化state_dict来加载模型参数字典\n",
    "\n",
    "### 什么是state_dict？\n",
    "在pytorch中，模型的可学习参数包含在模型的参数中。state_dict是Python字典，它将每一层映射到其参数张量上。注意只有具有可学习参数的模型才有state_dict，目前优化torch.optim也有state_dict属性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state dict\n",
      "conv1.weight torch.Size([6, 3, 5, 5])\n",
      "conv1.bias torch.Size([6])\n",
      "conv2.weight torch.Size([16, 6, 5, 5])\n",
      "conv2.bias torch.Size([16])\n",
      "fc1.weight torch.Size([120, 400])\n",
      "fc1.bias torch.Size([120])\n",
      "fc2.weight torch.Size([84, 120])\n",
      "fc2.bias torch.Size([84])\n",
      "fc3.weight torch.Size([10, 84])\n",
      "fc3.bias torch.Size([10])\n",
      "Optimizer's state dict\n",
      "state {}\n",
      "param_groups [{'lr': 0.0001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [4424580944, 4424579024, 4424577344, 4424579824, 4424258256, 4424260816, 4424659104, 4424578304, 4424259776, 4424259216]}]\n"
     ]
    }
   ],
   "source": [
    "# 举例说明state_dict的用法\n",
    "class TheModelClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TheModelClass, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3, 6, 5)\n",
    "        self.pool = torch.nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = torch.nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = torch.nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = torch.nn.Linear(120, 84)\n",
    "        self.fc3 = torch.nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = TheModelClass()\n",
    "\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=1e-4, momentum=0.9)\n",
    "\n",
    "print(\"Model's state dict\")\n",
    "for param_name in model.state_dict():\n",
    "    print(param_name, model.state_dict()[param_name].size())\n",
    "    \n",
    "print(\"Optimizer's state dict\")\n",
    "for param_name in optimizer.state_dict():\n",
    "    print(param_name, optimizer.state_dict()[param_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在保存模型的时候，有多种方法，可以只存储模型的各种参数(Pytorch官方也是推荐这种方式)，也可以将整个模型序列化到一个文件中。\n",
    "### save/load state_dict(suggested)\n",
    "使用这种方法的好处是，只保存模型的参数，反序列化的时候不依赖过多的类文件。请记住，在运行推理之前，务必调用model.eval()去设置 dropout 和 batch normalization 层为评估模式。如果不这么做，可能导致 模型推断结果不一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TheModelClass(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"example.pt\"\n",
    "# save\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "# load\n",
    "model = TheModelClass()\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sava/load model completely\n",
    "这个方法保存和加载过程需要的代码都极少。但是这个方法受限于序列化数据依赖某特殊的类而且需要确切的字典结构。这是因为pickle无法保存模型类本身，相反，它保存包含类文件的路径，所以在项目重构之后，可能出现模型加载失败"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TheModelClass(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save\n",
    "torch.save(model, PATH)\n",
    "# load\n",
    "model = torch.load(PATH)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save checkpoint\n",
    "这是预训练模型常用的做法，通过设置记录点的方式，将训练模型过程中的所有参数都保存下来，包括model.state_dict(),optimizer.state_dict(),loss,epoch等等。\n",
    "<br>\n",
    "\n",
    "```python\n",
    "# save\n",
    "torch.save({\n",
    "    \"epoch\": epoch,\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": model.state_dict(),\n",
    "    \"loss\": loss\n",
    "})\n",
    "\n",
    "# load \n",
    "model = TheModelClass()\n",
    "optimizer = TheOptimizerClass()\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chenguang",
   "language": "python",
   "name": "chenguang"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
