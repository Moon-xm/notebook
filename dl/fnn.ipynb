{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前馈神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简介\n",
    "\n",
    "本文内容总结自邱锡鹏老师的[《神经网络与深度学习》](https://nndl.github.io/)\n",
    "\n",
    "\n",
    "在前馈神经网络中，不同的神经元属于不同的层，每一层的神经元可以接受到前一层的神经元信号，并产生信号输出到下一层。第0层叫做输入层，最后一层叫做输出层，中间的叫做隐藏层，整个网络中无反馈，信号从输入层到输出层单向传播，可用一个有用无环图表示。\n",
    "\n",
    "![FNN](imgs/fnn1.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义符号含义如下:\n",
    "- L  神经网络的层数\n",
    "- $M_l$  第$l$层神经元的个数\n",
    "- $f_l(\\cdot)$  第$l$层神经元的激活函数\n",
    "- $W^{(l)} \\in R^{M_l\\times M_{l-1}}$  第$l-1$层到第𝑙层的权重矩阵\n",
    "- $b^{(l)} \\in R^{M_l}$第$l-1$层到第𝑙层的偏置\n",
    "- $z^{(l)} \\in R^{M_l}$第$l-1$层神经元的净输入(净活性值)\n",
    "- $a^{(l)} \\in R^{M_l}$第$l-1$层神经元的输出(活性值)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "令$a^{(0)} = x$，前馈神经网络通过不断迭代下面公式进行信息传播:\n",
    "$$\n",
    "z^{(l)} = W^{(l)}a^{(l-1)} + b^{(l)}\\tag{1}\n",
    "$$\n",
    "$$\n",
    "a^{(l)} = f_l(z^{(l)})\\tag{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将公式(1),(2)联合可以写作\n",
    "$$\n",
    "a^{(l)} = f_l(W^{(l)}a^{(l-1)} + b^{(l)})\\tag{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过公式(3)或者公式(1),(2)联合计算，就能从输入$x$得到模型的输出$\\hat{y}$，这个输出可以使矢量也可以是标量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定一个样本$(x,y)$，假设损失函数为$Loss(y, \\hat{y})$。参数优化的时候需要计算所有参数关于损失的梯度，然后使用梯度下降法对参数进行更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处求梯度涉及到[矩阵求导](../math/derivative.ipynb)。\n",
    "\n",
    "对第$l$层的参数$W^{(l)}$和$b^{(l)}$计算偏导数\n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial W^{(l)}} = \\frac{\\partial z^{(l)}}{\\partial W^{(l)}}\\frac{\\partial Loss}{\\partial z^{(l)}}\\tag{4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial Loss}{\\partial b^{(l)}} = \\frac{\\partial z^{(l)}}{\\partial b^{(l)}}\\frac{\\partial Loss}{\\partial z^{(l)}}\\tag{5}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "公式(4),(5)中第二项都是都是损失函数关于$z^{(l)}$的偏导数。令\n",
    "$$\n",
    "\\delta^{(l)} = \\frac{\\partial Loss}{\\partial z^{(l)}}\\tag{6}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么，根据公式(1), (2)，可以将公式(6)展开成\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\delta^{(l)} =&\n",
    "\\frac{\\partial a^{(l)}}{\\partial z^{(l)}}\n",
    "\\frac{\\partial z^{(l+1)}}{\\partial a^{(l)}}\n",
    "\\frac{\\partial Loss}{\\partial z^{(l+1)}}\\\\\n",
    "=& f_l'(z^{(l)})\\odot \\left((W^{(l + 1)})^T\\delta^{(l + 1)} \\right)\n",
    "\\end{aligned}\\tag{7}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "综上，公式(4), (5), (6),(7)得到\n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial W^{(l)}} = \\delta^{(l)}(a^{(l-1)})^T\\tag{8}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial Loss}{\\partial b^{(l)}} = \\delta^{(l)}\\tag{9}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前馈神经网络的前向后向计算方式可以用如上九个公式表示。本笔记也使用numpy实现了一份[FNN的代码](../code/dl/1_fnn.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
