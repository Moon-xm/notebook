{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "正则化的目的是为了防止模型过拟合，过拟合的模型在训练集上表现优异但是测试集上却不理想。构造模型最终目的是让模型在面对新数据的时候，可以有很好的表现。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量范数\n",
    "### 定义\n",
    "\n",
    "如果向量$X\\in R^n$的某个实值函数$f(X) = \\|X\\|$满足：\n",
    "\n",
    "1. 正定性: $\\|X\\| \\ge 0$, 当且仅当$X = 0$时$\\|X\\| = 0$\n",
    "2. 齐次性: 对任意认识$\\alpha$, 都有$\\|\\alpha X\\| = |\\alpha|\\ \\|X\\|$\n",
    "3. 三角不等式: 对任意$x, y \\in R^n$, 都有$\\|x + y\\| < \\|x\\| + \\|y\\|$\n",
    "\n",
    "则称$\\|X\\|$为$R^n$上的一个向量范数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L-P范数\n",
    "\n",
    "L-P范数不是一个范数，而是一组范数，其定义如下： \n",
    "$$\n",
    "L_p = \\|X\\|_p = \\displaystyle\\sqrt[p]{\\sum_{i = 1}^n|x_i|^p}, X = (x_1, x_2,\\ldots, x_n)\\tag{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1范数\n",
    "\n",
    "根据公式(1)的定义，L1范数可以写成如下形式\n",
    "$$\n",
    "L1 = \\|X\\|_1 = \\displaystyle\\sum_{i = 1}^n|x_i|\\tag{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2范数\n",
    "\n",
    "2-范数也称为Euclid范数（欧几里得范数，常用计算向量长度），即向量元素绝对值的平方和再开方，表示x到零点的欧式距离\n",
    "$$\n",
    "L2 = \\|X\\|_2 = \\sqrt{\\displaystyle\\sum_{i = 1}^nx_i^2}\\tag{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L$\\infty$范数\n",
    "\n",
    "$$\n",
    "L\\infty = \\|X\\|_{\\infty} = \\underset{1\\le i \\le n}{\\max}|x_i|\\tag{4}\n",
    "$$\n",
    "\n",
    "当L-P范数中的P趋于正无穷大的时候，通过极限求解(高数内容)可以证明公式(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L0范数\n",
    "\n",
    "$$\n",
    "L0 = \\|X\\|_0 = \\displaystyle\\sum_{i = 1}^nI(x_i \\not= 0)\\tag{5}\n",
    "$$\n",
    "\n",
    "L0范数不满足三角不等式和齐次性，故L0不是严格的范数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用范数正则化\n",
    "通常，我们假设模型的损失函数为$L = f(\\theta)$\n",
    "\n",
    "常见的几种正则化方法如下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1-Norm\n",
    "\n",
    "使用L1正则的时候，损失函数变成如下形式\n",
    "$$\n",
    "L = f(\\theta) + C\\|\\theta\\|_1 \\tag{6}\n",
    "$$\n",
    "其中的$C$是一个超参数。L1正则可以让模型产生一个稀疏解，这个在特征选择工程的时候有很大帮助。\n",
    "\n",
    "产生系数解的原因如下,对于公式(6)，是等价于\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\underset{\\theta}{arg\\ min} f(\\theta)\\\\\n",
    "&s.t.\\ \\|\\theta\\|_1 \\le \\eta\n",
    "\\end{aligned}\n",
    "\\tag{7}\n",
    "$$\n",
    "通过图像法，可以比较容易观测到解出现在约束条件的顶点。还可以参考[Likehood](../math/likehood.ipynb)\n",
    "\n",
    "![L1-norm](imgs/re1.jpeg)\n",
    "\n",
    "L1正则还有一个特点，在0出不可导，对梯度下降法求解问题造成极大困扰，通常使用如下的公式代替\n",
    "$$\n",
    "\\|\\theta\\|_1 = \\displaystyle\\sum_{d}^D\\sqrt{\\theta^2_d + \\epsilon} \\tag{8}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2-Norm\n",
    "\n",
    "使用L2正则的时候，损失函数变成如下形式\n",
    "$$\n",
    "L = f(\\theta) + C\\|\\theta\\|^2_2 \\tag{9}\n",
    "$$\n",
    "它可以将模型的参数限制在一个较小的范围，比如在SVM或者Logistic Regression中，用来分类的线性函数的参数会随着迭代次数多增加变得越来越大。可以证明，公式(9)等价于\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\underset{\\theta}{arg\\ min} f(\\theta)\\\\\n",
    "&s.t.\\ \\|\\theta\\|_2^2 \\le \\eta\n",
    "\\end{aligned}\n",
    "\\tag{10}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "dropout 是一种计算方便但功能强大的正则化方法，适用于神经网络。其基本步骤是在每一次的迭代中，随机使一部分节点失效，只训练剩下的节点。每次迭代都会随机删除，每次迭代删除的节点也都不一样，相当于每次迭代训练的都是不一样的网络，通过这样的方式降低节点之间的关联性以及模型的复杂度，从而达到正则化的效果。这点上有点类似 bagging，但是远比 bagging 来的简单。\n",
    "\n",
    "dropout只需要设置一个超参数 keep_prob，这个参数的意义是每层节点随机保留的比例，比如将 keep_prob 设置为 0.7，那么就会随机30%的节点消失，消失这个动作其实很简单，只是将这一层的参数矩阵与根据 keep_prob 生成的 {0, 1} 矩阵做 **逐点乘积**，当然前向传播与反向传播都需要做以上的操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设随机变量$\\xi_i$取值为0和1的概率分别为p和1-p，那么有\n",
    "$$E(\\xi_i) = 1-p$$<br>\n",
    "dropout的处理公式为\n",
    "$$h'_i = \\frac{\\xi_i}{1 -p}h_i$$,\n",
    "则$E(h'_i) = \\frac{E(\\xi_i)}{1 -p}h_i = h_i$\n",
    "dropout的缺点在于，需要将训练集分为不同子集输入每一次的迭代，这就需要较大的训练集，所以在训练集较小的情况下，dropout的效果可能并不好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "\n",
    "提前终止可能是最简单的正则化方式，他适用于模型的表达能力很强的时候。这种情况下，一般训练误差会随着训练次数的增多逐渐下降，而测试误差则会先下降而后再次上升。我们需要做的就是在测试误差最低的点停止训练即可。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 增大训练集\n",
    "\n",
    "更大数量的训练集是提升机器学习模型泛化能力最好的方法。在实际项目中，获取训练数据的成本会很高，这时候就需要我们自己来“创造”数据。希望在以后，GAN可以成功的应用到训练集增强领域。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
