{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "朴素贝叶斯算法是基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入输出的联合概率分布；然后基于此模型，对给定的输入$x$，利用贝叶斯定理求出后验概率醉的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设输入空间$\\chi \\subseteq R^m$为m维向量的集合，输出空间为类标记集合$L = \\{c_1, c_2, \\ldots, c_K\\}$。输入为特征向量$x\\in \\chi$，输出为类标记$y\\in L$。$X$是定义在输入空间$\\chi$上的随机向量，$Y$是定义在输出空间$L$上的随机变量。$P(X, Y)$是$X$和$Y$的联合概率分布。训练数据集\n",
    "$$\n",
    "T = \\{(x_1, y_1), (x_2, y_2), \\ldots, (x_N, y_N)\\}\n",
    "$$\n",
    "是由$P(X, Y)$独立同分布产生。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一个分类算法，我们要求一个使得$P(Y|X)$最大的类别。根据贝叶斯定理\n",
    "$$\n",
    "P(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)}\\tag{1}\n",
    "$$\n",
    "其中$P(X)$是一个常数，我们不能改变，$P(Y)$也极易估计，但是对于$P(X|Y)$，基本不存在一个数据集可以穷尽所有X，那么就无法直接估计$P(X|Y)$.NB算法提出了一个条件独立性假设，其具体的表示如下\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(X = x|Y = c_k) &= P(X^{(1)} = x^{(1)}, X^{(2)} = x^{(2)},\\ldots ,X^{(m)} = x^{(m)}|Y = c_k )\\\\\n",
    "&= \\displaystyle\\prod_{j = 1}^mP(X^{(j)} = x^{(j)}|Y = c_k)\n",
    "\\end{aligned}\\tag{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "naive Beyes 分类的时候，对给定的输入$x$，通过学习到的模型参数计算后验概率分布$P(Y = c_k|X = x)$，将后验概率最大的类作为$x$的类输出。\n",
    "$$\n",
    "y = \\arg\\underset{c_k}{\\max}P(Y = c_k)\\displaystyle\\prod_jP(X^{(j)} = x^{(j)}|Y = c_k)\\tag{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 后验概率最大化的意义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "朴素贝叶斯法将实例分类为后验概率最大的类。这等价于期望风险最小化。假设0-1损失函数\n",
    "$$\n",
    "L(Y, f(X)) = \n",
    "\\begin{cases}\n",
    "1, & Y \\not=f(X)\n",
    "\\\\\n",
    "0, & y = f(X)\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上式$f(X)$是分类决策函数。这时，期望风险函数为\n",
    "$$\n",
    "\\begin{aligned}\n",
    "R_{exp}(f) &= E\\big[L\\big(Y, f(X)\\big)\\big]\\\\\n",
    "& = E_X\\displaystyle\\sum_{k = 1}^K\\big[L(c_k, f(X))\\big]P(c_k|X)\n",
    "\\end{aligned}\n",
    "$$\n",
    "为了使期望风险最小化，只需要对$X = x$逐个最小化，由此得到:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(x) &= \\arg\\underset{y\\in L}{\\min}\\displaystyle\\sum_{k = 1}^KL(c_k, y)P(c_k|X = x)\\\\\n",
    "& = \\arg\\underset{y\\in L}{\\min}\\displaystyle\\sum_{k = 1}^KP(y\\not =c_k|X = x)\\\\\n",
    "& = \\arg\\underset{y\\in L}{\\min}\\big(1 - P(y =c_k|X = x)\\big)\\\\\n",
    "& = \\arg\\underset{y\\in L}{\\max} P(y =c_k|X = x)\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "这样一来，根据贝叶斯决策论就得到了后验概率最大化准则\n",
    "$$\n",
    "f(x) = \\arg\\underset{c_k}{\\max}P(c_k|X = x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB是一种监督学习方法，通常使用MLE对其参数进行估计,我们先构造似然函数\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\theta) &= \\arg\\underset{\\theta}{\\max}\\displaystyle\\prod_{i = 1}^N P(y^i|x^i)\\\\\n",
    "&= \\arg\\underset{\\theta}{\\max}\\displaystyle\\prod_{i = 1}^N \\frac{P(x^i|y^i)P(y^i)}{P(x^i)}\\\\\n",
    "&= \\arg\\underset{\\theta}{\\max}\\displaystyle\\prod_{i = 1}^N P(x^i|y^i)P(y^i)\\\\\n",
    "\\end{aligned}\\tag{4}\n",
    "$$\n",
    "上式只是一个使用了贝叶斯公式求MLE的表达式，实际上NB有其特殊性，那就是他的特征无关性假设。我们假设输入的维度为M，每一个特征上的取值可能性为L种,s使用$n^i_{jl}$表示第i个样本的第j个特征的取值为l的次数，该值要么为0要么为1。上式可以改写成\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\theta) &= \\arg\\underset{\\theta}{\\max}\\displaystyle\\prod_{i = 1}^N \\prod_{j = 1}^MP(x^i_j|y^i) P(y^i)\\\\\n",
    "&= \\arg\\underset{\\theta}{\\max}\\log\\displaystyle\\prod_{i = 1}^N \\prod_{j = 1}^MP(x^i_j|y^i) P(y^i)\\\\\n",
    "&= \\arg\\underset{\\theta}{\\max}\\displaystyle\\sum_{i = 1}^N\\bigg[\\sum_{j = 1}^M\\log P(x^i_j|y^i) + \\log P(y^i)\\bigg]\\\\\n",
    "&= \\arg\\underset{\\theta}{\\max}\\displaystyle\\sum_{i = 1}^N\\bigg[\\sum_{j = 1}^M\\sum_{l=1}^Ln^i_{jl}\\log P(x^i_j = a_{jl}|y^i) + \\log P(y^i)\\bigg]\\\\\n",
    "&= \\arg\\underset{\\theta}{\\max}\\displaystyle\\sum_{k =1}^K\\sum_{i:y^i = c_k}^N\\bigg[\\sum_{j = 1}^M\\sum_{l=1}^Ln^i_{jl}\\log \\theta_{kjl} + \\log \\theta_k\\bigg]\\\\\n",
    "\\end{aligned}\\tag{5}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加上约束，构成一个凸优化问题\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max\\ &\\displaystyle\\sum_{k =1}^K\\sum_{i:y^i = c_k}^N\\bigg[\\sum_{j = 1}^M\\sum_{l=1}^Ln^i_{jl}\\log \\theta_{kjl} + \\log \\theta_k\\bigg]\\\\\n",
    "s.t.\\ &\\sum_{k = 1}^K\\theta_k = 1\\\\\n",
    "&\\sum_{l = 1}^{L}\\theta_{kjl} = 1\n",
    "\\end{aligned}\\tag{6}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过拉格拉日乘子法解得\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\theta_k &= \\frac{\\displaystyle\\sum_{i = 1}^NI(y^i = c_k)}{N},  \\ \\ \\ k = 1,2, \\ldots, K\\\\\n",
    "\\theta_{kjl} &= \\frac{\\displaystyle\\sum_{i = 1}^NI(x^i_j = a_{jl}, y^i = c_k)}\n",
    "{\\displaystyle\\sum_{i = 1}^NI(y^i = c_k)}\\\\\n",
    "&i = 1,2, \\ldots, M, \\ \\ l = 1, 2, \\ldots, L,\\ \\ k = 1,2, \\ldots, K\n",
    "\\end{aligned}\\tag{7}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中$P(X = a_{jl}|Y) = \\theta_{kjl}, P(Y = c_k) = \\theta_k$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
